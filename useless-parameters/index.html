<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Useless Parameters</title><meta name="generator" content="Org-mode"><meta name="author" content="Zhitao Gong"><link rel="stylesheet" type="text/css" href="http://gongzhitaao.org/orgcss/org.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script></head><body><div id="content"><header><h1 class="title">Useless Parameters</h1></header><div class="abstract"><p>In this article, I empirically evaluate the impact of disturbing (i.e., randomly shuffling/re-initializing) the carefully selected parameters in neural network. And surprisingly, I got some <i>intriguing</i> results: a) biases do not matter as long as they remain small; b) useless weights do not matter.</p></div><div id="outline-container-orgheadline1" class="outline-2"><h2 id="orgheadline1"><span class="section-number-2">1</span> Introduction</h2><div class="outline-text-2" id="text-1"><p>A neural network usually has a large number of parameters [<a href="#han2015-1">1</a>].</p><table><colgroup><col class="org-left"><col class="org-left"></colgroup><thead><tr><th scope="col" class="org-left">Network</th><th scope="col" class="org-left">Parameters</th></tr></thead><tbody><tr><td class="org-left">LetNet-300-100</td><td class="org-left">1070KB</td></tr><tr><td class="org-left">LetNet-5</td><td class="org-left">1729KB</td></tr><tr><td class="org-left">AlexNet</td><td class="org-left">240MB</td></tr><tr><td class="org-left">VGG-16</td><td class="org-left">552MB</td></tr></tbody></table><p>It is really hard to imagine that changing a single weight would have a serious impact on the whole model. The interesting question is how much is too much.</p><p>There are many ways to compress a network [<a href="#iandola2016">2</a>,<a href="#han2015-1">1</a>]. Some could achieve astonishing compression ratio without losing accuracy. Here we do not use any advanced techniques, simply resetting the raw parameters to see how much impact each parameter may have.</p></div></div><div id="outline-container-orgheadline7" class="outline-2"><h2 id="orgheadline7"><span class="section-number-2">2</span> Toy Experiment</h2><div class="outline-text-2" id="text-2"><p>In this toy experiment, I use a simple 3-layer fully connected feed-forward network.</p><ul class="org-ul"><li>Data set: MNIST</li><li>Layer sizes: (28x28, 100, 100, 10)</li><li>Activation: sigmoid for hidden layers and softmax for output.</li><li>Learning algorithm: Adagrad</li><li>Loss function: categorical crossentropy</li><li>Platform: Keras.</li><li>Result: after 100 epochs, test accuracy is around 97%.</li></ul></div><div id="outline-container-orgheadline2" class="outline-3"><h3 id="orgheadline2"><span class="section-number-3">2.1</span> Parameter distribution</h3><div class="outline-text-3" id="text-2-1"><p>Figure <a href="#orgparagraph1">1</a> shows the parameter distribution of each layer. Left are weight distribution, right bias distribution. Top are distribution before training, bottom after training.</p><figure id="orgparagraph1"><p><img src="img/fc100-100-10-weight-per-layer.png" alt="fc100-100-10-weight-per-layer.png"></p><figcaption><span class="figure-number">Figure 1:</span> Parameter distribution of each layer.</figcaption></figure><p>The parameters are centered around zero, as in many other reports.</p><p>A few things to note:</p><ul class="org-ul"><li>Before training (epoch 0), as layers go deeper, the weights are more dispersed which is as expected since Keras by default uses <a href="https://github.com/fchollet/keras/blob/master/keras/initializations.py#L50">glorot algorithm</a> to initialize the weight.</li><li>It is interesting that the distribution in output layer<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> are bimodal, even before training. Are we going to see multi-modal distribution for deeper layers?</li><li>The weight distribution of each layer does not change much, if any. So what is being learned during training? Does it imply that we could avoid the training at all by a <i>clever</i> initialization?</li></ul></div></div><div id="outline-container-orgheadline3" class="outline-3"><h3 id="orgheadline3"><span class="section-number-3">2.2</span> How much being learned?</h3><div class="outline-text-3" id="text-2-2"><p>Consider a single weight, before training its value is randomly initialized to \(w\), and after training the value changes to \(w^\prime\). I define the <i>change scale</i> \(c\) as</p><p>\[c = \left\vert\frac{w^\prime - w}{w}\right\vert\times 100\%\]</p><p>Figure <a href="#orgparagraph2">2</a> shows the density plot of change scale for each layer. Left is weight matrix, right bias. Top is the absolute change, i.e., \(w^\prime - w\), bottom the change scale.</p><figure id="orgparagraph2"><p><img src="img/fc100-100-10-weight-diff-kde.png" alt="fc100-100-10-weight-diff-kde.png"></p><figcaption><span class="figure-number">Figure 2:</span> How much being learned?</figcaption></figure><p>Figure <a href="#orgparagraph3">3</a> shows the box plot of change scale.</p><figure id="orgparagraph3"><p><img src="img/fc100-100-10-weight-diff-box.png" alt="fc100-100-10-weight-diff-box.png"></p><figcaption><span class="figure-number">Figure 3:</span> The actual learner?</figcaption></figure><p>Those dots in Figure <a href="#orgparagraph3">3</a> are <i>outliers</i>. i.e., weights that changed dramatically.</p><p>From the above two plots of change scale, we could see that a portion of the weights do not change much. There are however some outliers that changed dramatically. So my hypothesis is that the improvement of the whole network accuracy is due to those outliers. To empirically study the hypothesis, we could randomly disturb those useless parameters, i.e., we can randomly <i>shuffle</i> or <i>re-initialize</i> to see the impact on accuracy.</p></div></div><div id="outline-container-orgheadline4" class="outline-3"><h3 id="orgheadline4"><span class="section-number-3">2.3</span> Disturb biases</h3><div class="outline-text-3" id="text-2-3"><p>The empirical conclusion is that biases does not matter in the final evaluation so long as they remain relatively small, which means they don't learn anything during the training process. We probably need to rethink the role biases play in deep network.</p><ol class="org-ol"><li><p>Random shuffling biases does not affect accuracy, if any. For each layer the evaluation is repeated 100 times. The result is show in Table <a href="#orgtable1">1</a><sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>.</p><table id="orgtable1"><caption class="t-above"><span class="table-number">Table 1:</span> Shuffling Biases</caption><colgroup><col class="org-left"><col class="org-left"></colgroup><thead><tr><th scope="col" class="org-left">&nbsp;</th><th scope="col" class="org-left">Accuracy after shuffling</th></tr></thead><tbody><tr><td class="org-left">Layer 0</td><td class="org-left">0.9721 (std: 0.0005204)</td></tr><tr><td class="org-left">Layer 1</td><td class="org-left">0.9749 (std: 0.0001272)</td></tr><tr><td class="org-left">Layer 2</td><td class="org-left">0.9747 (std: 0.0001808)</td></tr></tbody><tbody><tr><td class="org-left">All layer</td><td class="org-left">0.9720 (std: 0.0005995)</td></tr></tbody><tbody><tr><td class="org-left">Original accuracy</td><td class="org-left">0.9748</td></tr></tbody></table></li><li><p>Randomly resetting the biases <i>does</i> affect accuracy only if biases are too large. Small biases, however, do not make a difference, if any. During the experiment, the biases are randomly reset to the range\((0, U)\). I tested 100 experiments for each \(U\) value. The result is shown in Figure <a href="#orgparagraph4">4</a>. Specifically, resetting all biases to zeros does not affect accuracy at all.</p><figure id="orgparagraph4"><p><img src="img/fc100-100-10-reset-bias.png" alt="fc100-100-10-reset-bias.png"></p><figcaption><span class="figure-number">Figure 4:</span> Resetting Biases</figcaption></figure></li></ol></div></div><div id="outline-container-orgheadline5" class="outline-3"><h3 id="orgheadline5"><span class="section-number-3">2.4</span> Disturb weights</h3><div class="outline-text-3" id="text-2-4"><p>There are usually a large number of weights needed to be optimized in neural network models. Even in this simple 3-layer toy model, we have 98400 to find-tune. The <i>useless</i> weights being shuffled are selected by their change scale. The intuition is that those weights that do not change much during training are <i>useless</i> since they don't learn anything about the data.</p><p>Given a change scale threshold \(T\), the useless weights are those with change scale less than or equal to \(T\), i.e., \(c\leq T\). In the following plots, Y Axis is accuracy, X Axis is change scale.</p><ol class="org-ol"><li><p>Random shuffling useless weights does affect the accuracy only if we have too high a threshold. Figure <a href="#orgparagraph5">5</a> shows the impact of shuffling on accuracy against different threshold values. The top X Axis shows the percent of weights being shuffled.</p><figure id="orgparagraph5"><p><img src="img/fc100-100-10-shuffle-weight.png" alt="fc100-100-10-shuffle-weight.png"></p><figcaption><span class="figure-number">Figure 5:</span> Shuffling useless weights</figcaption></figure></li><li><p>I'm only interested in resetting weights to zero. As biases, resetting a small portion of the weights has a negligible impact on accuracy. Figure <a href="#orgparagraph6">6</a> shows the accuracy after resetting useless weights. \(U = 0\) corresponds to resetting weights to zero, \(U = 1\) resetting weights to random values in range \([0, 1)\).</p><figure id="orgparagraph6"><p><img src="img/fc100-100-10-reset-weight.png" alt="fc100-100-10-reset-weight.png"></p><figcaption><span class="figure-number">Figure 6:</span> Resetting useless weights</figcaption></figure></li></ol></div></div><div id="outline-container-orgheadline6" class="outline-3"><h3 id="orgheadline6"><span class="section-number-3">2.5</span> Final Model</h3><div class="outline-text-3" id="text-2-5"><p>Next we will see how much we could compress the model without a serious side affect on accuracy.</p><ol class="org-ol"><li>Reset all biases to zero</li><li>Reset weights in each layer separately with change scale less than 20%.</li></ol><p>The result is summarized in Figure <a href="#orgparagraph7">7</a>.</p><figure id="orgparagraph7"><p><img src="img/final_result.png" alt="final_result.png"></p><figcaption><span class="figure-number">Figure 7:</span> Compressed model</figcaption></figure><p>The lower X Axis shows the compress ratio, the upper X Axis shows the change scale criteria.</p></div></div></div><div id="outline-container-orgheadline8" class="outline-2"><h2 id="orgheadline8"><span class="section-number-2">3</span> Conclusion</h2><div class="outline-text-2" id="text-3"><p>Experiment on this toy example network shows that change scale is a simple yet efficient criteria to prune useless weights. However the compression ratio of just ~0.2 is simply trivial. Other work could achieve 50x compression without losing accuracy. An interesting side note is that biases does not matter much after training.</p></div></div><div id="bibliography"><h2>References</h2><table><tbody><tr valign="top"><td align="right" class="bibtexnumber">[<a name="han2015-1">1</a>]</td><td class="bibtexitem">Song Han, Huizi Mao, and William&nbsp;J. Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. <em>CoRR</em>, abs/1510.00149, 2015. [&nbsp;<a href="http://arxiv.org/abs/1510.00149">http</a>&nbsp;]</td></tr><tr valign="top"><td align="right" class="bibtexnumber">[<a name="iandola2016">2</a>]</td><td class="bibtexitem">Forrest&nbsp;N. Iandola, Matthew&nbsp;W. Moskewicz, Khalid Ashraf, Song Han, William&nbsp;J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 1mb model size. <em>CoRR</em>, abs/1602.07360, 2016. [&nbsp;<a href="http://arxiv.org/abs/1602.07360">http</a>&nbsp;]</td></tr></tbody></table></div><div id="footnotes"><h2 class="footnotes">Footnotes:</h2><div id="text-footnotes"><div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup><div class="footpara"><p class="footpara">By "output layer", I actually mean the layer before softmax, which has no parameters.</p></div></div><div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup><div class="footpara"><p class="footpara">"Layer X" means we only disturb parameters in layer X, leaving other layers unchanged. "All layers" means we disturb parameters across layers all at once. The same for all the other tables and figures.</p></div></div></div></div></div><div id="postamble" class="status"><a class="author" href="http://gongzhitaao.org">Zhitao Gong</a> / <span class="date">2016-06-25 Sat 14:27</span><span class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 24.5.1 (<a href="http://orgmode.org">Org</a> mode 8.3.4)</span></div></body></html>